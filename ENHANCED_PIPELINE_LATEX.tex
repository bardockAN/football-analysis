\documentclass[article]{article}

% ====== CÁC PACKAGE CẦN THIẾT ======
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx} % cho \includegraphics
\usepackage{amsmath,amssymb} % nếu sau này có công thức
\usepackage{geometry}
\geometry{margin=2.5cm}

\begin{document}

% GIẢ SỬ TRƯỚC ĐÓ BẠN ĐÃ CÓ SECTION 1, 2
% Ở DEMO NÀY MÌNH TĂNG COUNTER CHO SECTION
\setcounter{section}{2}  % => section tiếp theo là 3.xxx

\section{System Overview}

The proposed football analysis system employs a modular architecture
consisting of eight interconnected components that transform raw match
footage into quantitative performance metrics. The system operates on a
sequential pipeline paradigm where each module processes data from the
previous stage and passes enriched information to subsequent components,
ultimately producing annotated video output with comprehensive player
tracking, team identification, and performance analytics.

\subsection{Architectural Principles}

The system architecture adheres to four fundamental design principles.

\subsubsection{Modularity}

Each component is designed as an independent, encapsulated module with
well-defined input/output interfaces. This separation of concerns enables
individual components to be tested, validated, and potentially replaced
without affecting the overall system integrity. For instance, the object
detection module could be upgraded from YOLOv5 to YOLOv8 without modifying
downstream processing stages, provided the output format remains consistent.

\subsubsection{Unidirectional Data Flow}

Information flows sequentially through the pipeline in a single direction,
from raw video input through detection, tracking, spatial transformation,
and analysis, culminating in annotated visualization. This unidirectional
paradigm simplifies debugging, ensures reproducibility, and prevents
circular dependencies that could compromise system stability.

\subsubsection{Computational Efficiency}

The architecture incorporates caching mechanisms and optimization strategies
to minimize redundant computation. Intermediate results from computationally
intensive operations (object detection, optical flow calculation) are
serialized and cached, enabling rapid reprocessing during iterative
development and analysis workflows. This design reduces processing time by
approximately 85\% for subsequent runs on identical input data.

\subsubsection{Progressive Data Enrichment}

Rather than executing all analyses independently and merging results, the
system employs a progressive enrichment strategy where each module augments
a shared data structure with additional attributes. This approach maintains
data coherence, simplifies inter-module communication, and ensures that all
analytical outputs reference consistent object identities across the
temporal dimension.

\subsection{System Input and Output Specifications}

\subsubsection{Input Requirements}

The system operates on broadcast-style football match recordings that
satisfy the following conditions:
\begin{itemize}
    \item Video format: MP4, AVI, or MOV container formats
    \item Resolution: Minimum 720p (1280$\times$720), optimal at 1080p (1920$\times$1080)
    \item Frame rate: 24--30 FPS (standard broadcast specifications)
    \item Camera perspective: Wide-angle pitch view capturing majority of playing field
    \item Duration: Variable (tested on 60--180 second clips, scalable to full match footage)
\end{itemize}

\subsubsection{Output Deliverables}

The system produces both visual and numerical outputs, including:
\begin{itemize}
    \item Annotated video stream with frame-by-frame visual overlays
    \item Quantitative metrics: player speed (km/h), distance covered (meters), team possession percentage
    \item Visual annotations: color-coded team bounding boxes, ball possession indicators, player identification labels
    \item Performance statistics: per-player movement analytics, team-level possession metrics
\end{itemize}

\subsection{Core Component Architecture}

Figure~\ref{fig:pipeline} presents an overview of the football analysis
system pipeline architecture. The system processes match footage through
sequential stages: (1) Object Detection using YOLOv5, (2) parallel spatial
analysis including Camera Movement Tracking and Perspective Transformation,
(3) Speed \& Distance Calculation, (4) Team Assignment via K-Means
clustering, (5) Ball Possession Analysis, culminating in annotated video
output with player tracking, team identification, and performance metrics.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{System Architecture.png}
    \caption{Overview of the football analysis system pipeline architecture.
    The system processes match footage through sequential stages: (1) Object
    Detection using YOLOv5, (2) parallel spatial analysis including Camera
    Movement Tracking and Perspective Transformation, (3) Speed \& Distance
    Calculation, (4) Team Assignment via K-Means clustering, (5) Ball
    Possession Analysis, culminating in annotated video output with player
    tracking, team identification, and performance metrics.}
    \label{fig:pipeline}
\end{figure}

The system comprises eight core processing modules organized into four
functional layers:

\subsubsection{Layer 1: Detection and Tracking}

\paragraph{Object Detection Module:}
Employs YOLOv5x deep neural network for simultaneous detection of players,
referees, and ball across video frames. Operates on batch processing (20
frames per batch) with confidence threshold filtering (0.1 minimum
confidence) to balance detection sensitivity and false positive suppression.

\paragraph{Multi-Object Tracking Module:}
Implements CSRT (Discriminative Correlation Filter with Channel and Spatial
Reliability) tracking algorithm for maintaining object identity across
temporal sequences. Utilizes Euclidean distance matching (50-pixel threshold)
for track association and supports occlusion handling through 30-frame track
persistence.

\subsubsection{Layer 2: Spatial Analysis and Transformation}

\paragraph{Camera Movement Estimator:}
Applies Lucas-Kanade pyramidal optical flow to estimate inter-frame camera
displacement. Extracts features from static pitch elements (boundary lines)
and computes motion vectors to distinguish camera motion from object motion.

\paragraph{Position Adjustment Module:}
Compensates object trajectories for camera pan and tilt by subtracting
estimated camera displacement from raw positional data, yielding
camera-independent motion vectors.

\paragraph{Perspective Transformation Module:}
Converts pixel-based coordinates to real-world metric measurements through
homographic transformation. Maps pitch corners to standard field dimensions
(68m $\times$ 23.32m) enabling distance and speed quantification in
physically meaningful units.

\paragraph{Ball Position Interpolation Module:}
Addresses detection dropout (approximately 40\% of frames) through temporal
interpolation using surrounding valid detections, ensuring trajectory
continuity for possession analysis.

\subsubsection{Layer 3: Analytics and Classification}

\paragraph{Speed and Distance Estimator:}
Computes player velocities using sliding window differentiation (5-frame
window) over metric-transformed positions. Calculates cumulative distance
traveled and instantaneous speed with conversion to standard units (km/h for
speed, meters for distance).

\paragraph{Team Assignment Module:}
Identifies team membership through two-stage K-Means clustering on jersey
color features. Stage one separates foreground (jersey) from background;
stage two differentiates team jerseys based on dominant color clusters in
RGB space.

\paragraph{Ball Possession Analyzer:}
Determines ball control through proximity-based assignment (70-pixel
threshold) and aggregates frame-wise possession to compute team-level
possession statistics.

\subsubsection{Layer 4: Visualization and Output}

\paragraph{Annotation Rendering Module:}
Synthesizes analytical outputs into visual overlays including color-coded
bounding boxes, identification labels, speed/distance metrics, and possession
indicators. Renders annotations with anti-aliasing and alpha blending for
visual clarity.

\subsection{Data Structure and Information Flow}

The system maintains a centralized tracking data structure that serves as
the primary information carrier throughout the pipeline. This hierarchical
dictionary organizes data by object type (players, referees, ball), frame
number, and track identifier, with each entry containing progressively
enriched attributes as data flows through processing stages.

The information flow follows this sequence:
\begin{enumerate}
    \item Video frames are decomposed and processed through object detection
    \item Detected objects are assigned unique identifiers and tracked across frames
    \item Raw pixel positions are extracted from bounding box coordinates
    \item Camera movement is estimated and positions are compensated
    \item Compensated positions are transformed to metric coordinates
    \item Ball trajectories are smoothed through interpolation
    \item Player metrics (speed, distance) are calculated from metric positions
    \item Team assignments are determined through color clustering
    \item Ball possession is assigned based on proximity analysis
    \item All enriched data is rendered as visual annotations
    \item Annotated frames are encoded into output video
\end{enumerate}

This sequential enrichment ensures that each module operates on validated,
preprocessed data from upstream components, minimizing error propagation and
maintaining analytical coherence.

\subsection{Technology Stack}

The system is implemented using the following technology stack:
\begin{itemize}
    \item \textbf{Deep Learning Framework:} Ultralytics YOLOv5 for object detection
    \item \textbf{Computer Vision Library:} OpenCV 4.x for tracking, optical flow, perspective transformation, and rendering
    \item \textbf{Numerical Computing:} NumPy for array operations and geometric computations
    \item \textbf{Data Processing:} Pandas for time-series interpolation and statistical analysis
    \item \textbf{Machine Learning:} Scikit-learn for K-Means clustering in team assignment
    \item \textbf{Programming Language:} Python 3.8+ for system integration and orchestration
\end{itemize}

\subsection{Performance Characteristics}

The system demonstrates the following performance characteristics on standard
hardware (Intel Core i7 CPU, 16GB RAM):
\begin{itemize}
    \item \textbf{Processing Speed:} 15 minutes for 90-second video (2,160 frames) in initial run; 2--3 minutes with caching enabled
    \item \textbf{Detection Accuracy:} YOLOv5x achieves mean Average Precision (mAP) $>$0.85 on football-specific dataset
    \item \textbf{Tracking Persistence:} CSRT maintains identity through occlusions up to 30 frames (1.25 seconds at 24 FPS)
    \item \textbf{Ball Detection Rate:} $\sim$60\% direct detection, 100\% coverage through interpolation
    \item \textbf{Team Classification Accuracy:} $>$95\% consistent assignment after frame-zero calibration
    \item \textbf{Possession Assignment Precision:} 70-pixel threshold yields $<$5\% false positive rate
\end{itemize}

This system overview establishes the architectural foundation upon which the
detailed pipeline description elaborates in the following sections.

%-------------------------------------------------------------------
% SECTION 3.2 - FULL PIPELINE DESCRIPTION
%-------------------------------------------------------------------

\section{Full Pipeline Description}

The proposed football analysis system implements a sophisticated nine-stage
pipeline that transforms raw match footage into quantitative performance
analytics. Each stage performs specific computational tasks while maintaining
data coherence through a unified tracking data structure. The following
subsections detail the algorithmic implementation and data flow of each
pipeline component.

\subsection{Video Acquisition and Preprocessing}

The pipeline initiates with video acquisition, where match footage is
ingested through the \texttt{read\_video()} utility function. The input
video stream (24 FPS, 1920$\times$1080 resolution) is decomposed into
individual frames represented as NumPy arrays with shape $[H, W, 3]$ in BGR
color space. This frame-by-frame representation enables independent
processing at the image level while preserving temporal relationships
necessary for tracking algorithms. The preprocessing stage requires no
normalization or resizing, as the pretrained YOLO model accepts variable
input dimensions.

\subsection{Object Detection and Multi-Object Tracking}

The detection and tracking module employs a hybrid approach combining deep
learning-based detection with classical tracking algorithms. Initially, the
YOLOv5x model (\texttt{models/best.pt}) performs batch inference on video
frames with a batch size of 20 and confidence threshold of 0.1. The model
outputs three object classes: players, referees, and ball, with each
detection characterized by a bounding box $[x_1, y_1, x_2, y_2]$ and
confidence score.

For temporal association across frames, the system implements the CSRT
(Discriminative Correlation Filter with Channel and Spatial Reliability)
tracker from OpenCV. Upon initial detection, a CSRT tracker instance is
instantiated for each object with a unique identifier assigned from an
incrementing counter. The tracking association employs Euclidean distance
matching with a 50-pixel threshold between predicted and detected positions:
\begin{equation}
d_{\text{match}} = \sqrt{(x_{\text{det}} - x_{\text{pred}})^2 + (y_{\text{det}} - y_{\text{pred}})^2} < 50
\end{equation}

Lost tracks are maintained for up to 30 frames before deactivation, allowing
temporary occlusion recovery. The tracking output is structured as a
hierarchical dictionary containing frame-wise bounding box coordinates for
players, referees, and ball objects, indexed by unique track identifiers.

To optimize computational efficiency, the system implements stub caching via
serialization, storing detection and tracking results for reuse. This
mechanism reduces reprocessing time from approximately 15 minutes to 2
minutes for subsequent analyses.

\subsection{Camera Motion Estimation and Compensation}

Camera movement introduces artificial displacement in object trajectories,
confounding true player motion analysis. To address this, the system
implements the Lucas-Kanade optical flow algorithm with pyramidal refinement.

The algorithm initializes by extracting feature points from the first frame
using Shi-Tomasi corner detection with the following parameters:
\texttt{maxCorners}=100, \texttt{qualityLevel}=0.3, \texttt{minDistance}=3,
\texttt{blockSize}=7. Feature detection is constrained to pitch boundary
regions (columns 0--20 and 900--1050) via a binary mask, ensuring that
tracked features correspond to static field elements rather than moving
players.

For each subsequent frame, sparse optical flow is computed using
\texttt{cv2.calcOpticalFlowPyrLK()} with configuration:
\begin{itemize}
    \item Window size: 15$\times$15 pixels
    \item Pyramid levels: 2
    \item Termination criteria: $\varepsilon$=0.03 or 10 iterations
\end{itemize}

The algorithm tracks feature displacement between consecutive frames, with
camera movement estimated from the feature exhibiting maximum displacement:
\begin{equation}
\vec{c}_t = \arg\max_i \|\vec{f}_t^i - \vec{f}_{t-1}^i\|
\end{equation}
where $\vec{f}_t^i$ represents the position of feature $i$ at frame $t$.
Object positions are then adjusted by subtracting camera displacement:
\begin{equation}
\vec{p}_{\text{adjusted}} = \vec{p}_{\text{original}} - \vec{c}_t
\end{equation}

This compensation yields camera-independent trajectories essential for
accurate motion quantification. Results are cached in
\texttt{stubs/camera\_movement\_stub.pkl} for efficiency.

\subsection{Perspective Transformation}

To convert pixel-based measurements into real-world metric coordinates, the
system applies homographic perspective transformation. Four manually
annotated corner points on the pitch are mapped to their corresponding
real-world coordinates based on standard football field dimensions (68m
$\times$ 23.32m).

The transformation maps four manually annotated corner points from pixel
coordinates to their corresponding real-world coordinates based on standard
football field dimensions (68m $\times$ 23.32m). The homography matrix
$\mathbf{H}$ is computed solving the Direct Linear Transformation (DLT)
problem:
\begin{equation}
\begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \mathbf{H} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
\end{equation}
where $(x, y)$ are pixel coordinates and $(x', y')$ are metric coordinates.
Before transformation, point validity is verified using
\texttt{cv2.pointPolygonTest()} to ensure positions lie within the defined
pitch quadrilateral. Invalid points (e.g., outside field boundaries) are
assigned \texttt{None} to prevent erroneous calculations. This transformation
enables meaningful distance and speed measurements in standard units.

\subsection{Ball Position Interpolation}

Due to occlusion, motion blur, and small object size, ball detection exhibits
significant intermittency (approximately 40\% frame dropout). To ensure
trajectory continuity, the system employs pandas-based interpolation. Ball
positions across frames are extracted and organized into a DataFrame with
columns $[x_1, y_1, x_2, y_2]$. Missing values are filled using linear
interpolation via \texttt{DataFrame.interpolate()}, which estimates
intermediate positions from surrounding valid detections:
\begin{equation}
\text{bbox}_t = \text{bbox}_{t-k} + \frac{t - (t-k)}{(t+m) - (t-k)} \cdot (\text{bbox}_{t+m} - \text{bbox}_{t-k})
\end{equation}
where $k$ and $m$ are the nearest preceding and succeeding frames with valid
detections. Backward fill (\texttt{bfill()}) handles initial frame gaps by
propagating the first valid detection backward. This interpolation strategy
produces smooth ball trajectories essential for possession analysis.

\subsection{Speed and Distance Calculation}

Player speed is estimated using a sliding window approach over transformed
(metric) positions. For a window of 5 frames, displacement between the first
and last frame is computed:
\begin{equation}
\Delta d = \sqrt{(x_{t+5} - x_t)^2 + (y_{t+5} - y_t)^2}
\end{equation}

Time elapsed is calculated from the frame rate:
\begin{equation}
\Delta t = \frac{5}{24 \text{ FPS}} = 0.208 \text{ seconds}
\end{equation}

Instantaneous speed is derived and converted to km/h:
\begin{equation}
v = \frac{\Delta d}{\Delta t} \times 3.6 \quad [\text{km/h}]
\end{equation}

Cumulative distance is maintained by summing displacement across all windows
for each player. Speed and distance values are replicated across all frames
within each window to provide consistent annotations.

\subsection{Team Assignment via Color Clustering}

Team identification leverages jersey color analysis through a two-stage
K-Means clustering approach. For each player bounding box, the upper 50\%
region is extracted to isolate the jersey area while minimizing interference
from shorts and pitch background:
\begin{equation}
I_{\text{jersey}} = I_{\text{bbox}}[0:\lfloor h/2 \rfloor, :]
\end{equation}

\paragraph{Stage 1 -- Foreground Segmentation:}
K-Means clustering with $k=2$ separates jersey pixels from background. The
non-player cluster is identified by examining corner pixel labels, assuming
corners predominantly contain background:
\begin{equation}
C_{\text{background}} = \arg\max_{c \in \{0,1\}} \sum_{i \in \text{corners}} \mathbb{1}[\text{label}_i = c]
\end{equation}

\paragraph{Stage 2 -- Team Differentiation:}
Player colors from all detections in the first frame are aggregated and
clustered again with $k=2$ using the k-means++ initialization strategy and
\texttt{n\_init}=10 to ensure stable convergence. The resulting cluster
centers represent the dominant jersey colors of both teams:
\begin{equation}
\vec{\mu}_{\text{team}_1}, \vec{\mu}_{\text{team}_2} = \text{KMeans}(\{\vec{c}_p\}_{p=1}^{N}, k=2)
\end{equation}

Each player is assigned to the nearest team color in RGB space using
Euclidean distance. Team assignments persist across frames through a caching
mechanism (\texttt{player\_team\_dict}) to maintain consistency and reduce
computation.

\subsection{Ball Possession Assignment}

Ball possession is determined through proximity-based assignment. For each
frame, the distance from the ball center to each player's bounding box edges
(left and right) is computed:
\begin{equation}
d_L = \|(\text{bbox}_{x_{\min}}, \text{bbox}_{y_{\max}}) - \vec{p}_{\text{ball}}\|
\end{equation}
\begin{equation}
d_R = \|(\text{bbox}_{x_{\max}}, \text{bbox}_{y_{\max}}) - \vec{p}_{\text{ball}}\|
\end{equation}

The player with minimum distance below the threshold (70 pixels) is assigned
possession:
\begin{equation}
\text{player}_{\text{possess}} = \arg\min_i \min(d_L^i, d_R^i) \quad \text{subject to} \quad \min(d_L^i, d_R^i) < 70
\end{equation}

If no player satisfies the threshold, possession is maintained from the
previous frame to handle brief loss scenarios. Team-level possession
statistics are computed by aggregating frame-wise assignments:
\begin{equation}
\text{Possession}_{\text{team}} = \frac{\sum_{t=1}^T \mathbb{1}[\text{team}_t = \text{team}]}{T} \times 100\%
\end{equation}

\subsection{Visualization and Output Generation}

The final stage overlays analytical annotations onto video frames. The
visualization module renders:
\begin{itemize}
    \item Color-coded bounding boxes (team-specific RGB values)
    \item Player identification labels and team assignments
    \item Ball possession indicators with team color highlights
    \item Speed and distance metrics positioned below player feet
    \item Camera movement vectors illustrating frame-to-frame displacement
    \item Team possession statistics displayed as percentage bars
\end{itemize}

Annotations utilize OpenCV's \texttt{cv2.rectangle()}, \texttt{cv2.putText()},
and \texttt{cv2.circle()} functions with anti-aliased rendering. The annotated
frames are encoded into AVI format via \texttt{cv2.VideoWriter()} with MJPEG
codec and saved to \texttt{output\_videos/output\_video.avi}.

\subsection{Pipeline Integration and Data Flow}

The complete pipeline executes sequentially with each module enriching the
shared tracking data structure through the following stages: Input Video
$\rightarrow$ Detection/Tracking $\rightarrow$ Camera Compensation
$\rightarrow$ Perspective Transform $\rightarrow$ Ball Interpolation
$\rightarrow$ Speed/Distance Calculation $\rightarrow$ Team Assignment
$\rightarrow$ Ball Assignment $\rightarrow$ Visualization $\rightarrow$
Output Video.

This modular architecture ensures separation of concerns while maintaining
data coherence through the centralized tracking dictionary. The pipeline
processes a 90-second match clip (2,160 frames) in approximately 15 minutes
on CPU (Intel Core i7) or 3 minutes with stub caching enabled, demonstrating
practical applicability for match analysis workflows.

%-------------------------------------------------------------------
% EXECUTION SEQUENCE SUMMARY
%-------------------------------------------------------------------

\subsection{Execution Sequence Summary}

Based on the implementation in \texttt{main.py}, the precise execution order is:
\begin{enumerate}
    \item \textbf{Video Loading:} Read input video and decompose into frames
    \item \textbf{Object Detection:} YOLOv5x batch inference (batch\_size=20, conf=0.1)
    \item \textbf{Object Tracking:} CSRT tracker initialization and frame-to-frame association
    \item \textbf{Position Extraction:} Convert bounding boxes to positional coordinates
    \item \textbf{Camera Motion Estimation:} Lucas-Kanade optical flow on pitch boundaries
    \item \textbf{Position Adjustment:} Compensate object positions for camera movement
    \item \textbf{Perspective Transformation:} Convert pixel coordinates to metric (meters)
    \item \textbf{Ball Interpolation:} Fill missing ball detections via pandas interpolation
    \item \textbf{Speed \& Distance Calculation:} Compute metrics using 5-frame sliding window
    \item \textbf{Team Assignment:} K-Means clustering on jersey colors (frame 0 calibration)
    \item \textbf{Ball Possession Assignment:} Proximity-based assignment (70px threshold)
    \item \textbf{Annotation Rendering:} Draw bounding boxes, labels, and statistics
    \item \textbf{Video Export:} Save annotated frames to output video file
\end{enumerate}

%-------------------------------------------------------------------
% KEY PERFORMANCE METRICS
%-------------------------------------------------------------------

\subsection{Key Performance Metrics}

The system achieves the following performance metrics:
\begin{itemize}
    \item \textbf{Processing Time (No Cache):} $\sim$15 minutes for 90-second video (2,160 frames)
    \item \textbf{Processing Time (With Cache):} $\sim$2--3 minutes for subsequent runs
    \item \textbf{Ball Detection Rate:} $\sim$60\% (40\% frames require interpolation)
    \item \textbf{Tracking Accuracy:} CSRT maintains identity across 30-frame occlusions
    \item \textbf{Team Assignment Stability:} Frame 0 calibration ensures consistent classification
    \item \textbf{Speed Calculation Window:} 5 frames (0.208 seconds at 24 FPS)
    \item \textbf{Possession Threshold:} 70 pixels for ball-to-player assignment
    \item \textbf{Output Resolution:} 1920$\times$1080 (original resolution maintained)
\end{itemize}

%-------------------------------------------------------------------
% DATA STRUCTURE EVOLUTION
%-------------------------------------------------------------------

\subsection{Data Structure Evolution}

The tracking data structure evolves incrementally through the pipeline stages:

\paragraph{Stage 1--3 (Detection \& Tracking):}
Each tracked object is assigned a bounding box with coordinates.

\paragraph{Stage 4--5 (Position \& Camera):}
Original positions and camera-compensated positions are added to each track.

\paragraph{Stage 6--7 (Perspective \& Interpolation):}
Positions are transformed to real-world metric coordinates (meters).

\paragraph{Stage 8 (Speed \& Distance):}
Speed (km/h) and cumulative distance (meters) metrics are calculated and stored.

\paragraph{Stage 9--10 (Team \& Possession):}
Team identification, team colors, and ball possession status are assigned to player tracks.

This incremental enrichment pattern ensures data dependencies are satisfied at
each stage, with each module consuming outputs from previous stages and
producing inputs for subsequent processing.

\end{document}
