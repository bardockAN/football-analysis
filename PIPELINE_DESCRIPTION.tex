\subsubsection{Pipeline Execution}
Our pipeline processes videos through 12 sequential stages. First, we decompose the input video into frames at 1920$\times$1080 resolution and 24 FPS. These frames are then fed into YOLOv5x for object detection using a batch size of 20 and confidence threshold of 0.1 to maximize recall. The detected objects are tracked across frames using CSRT with Euclidean distance association (50px threshold) and 30-frame persistence to handle temporary occlusions.

Camera motion is estimated using Lucas-Kanade optical flow on Shi-Tomasi corner features extracted from static pitch boundaries. Player positions are compensated for camera movement via $\vec{p}_{\text{adj}} = \vec{p}_{\text{orig}} - \vec{c}_t$, where $\vec{c}_t$ is the camera displacement vector. Next, we apply homography transformation to map image coordinates $(x,y)$ to real-world field coordinates $(x',y')$ on a standard 68m $\times$ 23.32m pitch.

Ball trajectories often have missing detections, which we handle through interpolation. For gaps in the trajectory, we use pandas linear interpolation $y_t = y_{t-1} + \frac{y_{t+k} - y_{t-1}}{k+1}$ for intermediate frames, with backward fill $y_t = y_{t+1}$ applied to sequence endpoints where forward interpolation is not possible.

Player speeds are calculated as $v = \frac{\Delta d}{\Delta t} \times 3.6$ [km/h] using a sliding window approach. We chose a \textbf{5-frame window} because at 24 FPS, this gives us 0.208 seconds of data, which provides a good balance. Shorter windows (less than 3 frames) make the measurements too noisy due to the 1-2 pixel jitter inherent in tracking, while longer windows (more than 7 frames) smooth out important details like sudden accelerations or changes in direction.

Team assignment uses two-stage K-Means clustering on jersey color histograms extracted from player bounding boxes. Ball possession is determined by proximity, assigning the ball to the nearest player within \textbf{70 pixels}. This threshold was chosen based on practical considerations: player bounding boxes are typically 40-80px wide, the ball is 5-10px in radius, and when a player has possession, their foot is usually within 30px of the ball center. We added some margin for detection errors, giving us 70px total, which corresponds to roughly 1.5-2.0 meters in real-world distance. We found that thresholds below 50px miss legitimate possession during dribbling, while thresholds above 90px incorrectly assign the ball to nearby players who don't actually have it.

Finally, we render annotations using OpenCV drawing functions and export the result as an AVI video. Throughout the pipeline, data is organized in a nested dictionary structure \texttt{\{object\_type: \{frame\_id: \{track\_id: \{bbox, position, team, speed\}\}\}\}} that gets progressively enriched with more attributes at each stage. The initial processing takes about 15 minutes for a 90-second video (2,160 frames), but subsequent runs with cached detections complete in 2-3 minutes.
